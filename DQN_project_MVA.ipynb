{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may need to install [OpenCV](https://pypi.python.org/pypi/opencv-python) and [scikit-video](http://www.scikit-video.org/stable/).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "import skvideo.io\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "from keras.models import Sequential,Model,model_from_json\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, AveragePooling2D,Reshape,BatchNormalization, Input, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniProject on Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notations__: $E_p$ is the expectation under probability $p$. Please justify each of your answer and widely comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a reinforcement learning algorithm, we modelize each step $t$ as an action $a_t$ obtained from a state $s_t$, i.e. $\\{(a_{t},s_{t})_{t\\leq T}\\}$ having the Markov property. We consider a discount factor $\\gamma \\in [0,1]$ that ensures convergence. The goal is to find among all the policies $\\pi$, one that maximizes the expected reward:\n",
    "\n",
    "\\begin{equation*}\n",
    "R(\\pi)=\\sum_{t\\leq T}E_{p^{\\pi}}[\\gamma^t r(s_{t},a_{t})] \\> ,\n",
    "\\end{equation*}\n",
    "\n",
    "where: \n",
    "\\begin{equation*}p^{\\pi}(a_{0},a_{1},s_{1},...,a_{T},s_{T})=p(a_{0})\\prod_{t=1}^{T}\\pi(a_{t}|s_{t})p(s_{t+1}|s_{t},a_{t}) \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "We note the $Q$-function:\n",
    "\n",
    "\\begin{equation*}Q^\\pi(s,a)=E_{p^{\\pi}}[\\sum_{t\\leq T}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a] \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "Thus, the optimal Q function is:\n",
    "\\begin{equation*}\n",
    "Q^*(s,a)=\\max_{\\pi}Q^\\pi(s,a) \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "In this project, we will apply the deep reinforcement learning techniques to a simple game: an agent will have to learn from scratch a policy that will permit it maximizing a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The environment, the agent and the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Environment``` is an abstract class that represents the states, rewards, and actions to obtain the new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, act):\n",
    "        \"\"\"\n",
    "        One can act on the environment and obtain its reaction:\n",
    "        - the new state\n",
    "        - the reward of the new state\n",
    "        - should we continue the game?\n",
    "\n",
    "        :return: state, reward, game_over\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reinitialize the environment to a random state and returns\n",
    "        the original state\n",
    "\n",
    "        :return: state\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        Visualize in the console or graphically the current state\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method ```act``` allows to act on the environment at a given state $s_t$ (stored internally), via action $a_t$. The method will return the new state $s_{t+1}$, the reward $r(s_{t},a_{t})$ and determines if $t\\leq T$ (*game_over*).\n",
    "\n",
    "The method ```reset``` simply reinitializes the environment to a random state $s_0$.\n",
    "\n",
    "The method ```draw``` displays the current state $s_t$ (this is useful to check the behavior of the Agent).\n",
    "\n",
    "We modelize $s_t$ as a tensor, while $a_t$ is an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the ```Agent``` is to interact with the ```Environment``` by proposing actions $a_t$ obtained from a given state $s_t$ to attempt to maximize its __reward__ $r(s_t,a_t)$. We propose the following abstract class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, epsilon=0.1, n_action=4):\n",
    "        self.epsilon = epsilon\n",
    "        self.n_action = n_action\n",
    "    \n",
    "    def set_epsilon(self,e):\n",
    "        self.epsilon = e\n",
    "\n",
    "    def act(self,s,train=True):\n",
    "        \"\"\" This function should return the next action to do:\n",
    "        an integer between 0 and 4 (not included) with a random exploration of epsilon\"\"\"\n",
    "        if train:\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                a = np.random.randint(0, self.n_action, size=1)[0]\n",
    "            else:\n",
    "                a = self.learned_act(s)\n",
    "        else: # in some cases, this can improve the performance.. remove it if poor performances\n",
    "            a = self.learned_act(s)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def learned_act(self,s):\n",
    "        \"\"\" Act via the policy of the agent, from a given state s\n",
    "        it proposes an action a\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reinforce(self, s, n_s, a, r, game_over_):\n",
    "        \"\"\" This function is the core of the learning algorithm. \n",
    "        It takes as an input the current state s_, the next state n_s_\n",
    "        the action a_ used to move from s_ to n_s_ and the reward r_.\n",
    "        \n",
    "        Its goal is to learn a policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" This function returns basic stats if applicable: the\n",
    "        loss and/or the model\"\"\"\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\" This function allows to restore a model\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Question 1__:\n",
    "Explain the function act. Why is ```epsilon``` essential?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function act has to mode of action, one during training and one during test. In train we want to use the learned policy to check its efficiency but we also want to allow the model to try and learn new policies thus under a certain probability treshold defined by $\\epsilon$ we randomly output a new behaviour. In test we simply output the learned policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### The Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Agent``` and the ```Environment``` work in an interlaced way as in the following (take some time to understand this code as it is the core of the project)\n",
    "\n",
    "```python\n",
    "\n",
    "epoch = 300\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "\n",
    "# Number of won games\n",
    "score = 0\n",
    "loss = 0\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    # At each epoch, we restart to a fresh game and get the initial state\n",
    "    state = env.reset()\n",
    "    # This assumes that the games will end\n",
    "    game_over = False\n",
    "\n",
    "    win = 0\n",
    "    lose = 0\n",
    "    \n",
    "    while not game_over:\n",
    "        # The agent performs an action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Apply an action to the environment, get the next state, the reward\n",
    "        # and if the games end\n",
    "        prev_state = state\n",
    "        state, reward, game_over = env.act(action)\n",
    "\n",
    "        # Update the counters\n",
    "        if reward > 0:\n",
    "            win = win + reward\n",
    "        if reward < 0:\n",
    "            lose = lose -reward\n",
    "\n",
    "        # Apply the reinforcement strategy\n",
    "        loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "\n",
    "    # Save as a mp4\n",
    "    if e % 10 == 0:\n",
    "        env.draw(e)\n",
    "\n",
    "    # Update stats\n",
    "    score += win-lose\n",
    "\n",
    "    print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "          .format(e, epoch, loss, win, lose, win-lose))\n",
    "    agent.save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The game, *eat cheese*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rat runs on an island and tries to eat as much as possible. The island is subdivided into $N\\times N$ cells, in which there are cheese (+0.5) and poisonous cells (-1). The rat has a visibility of 2 cells (thus it can see $5^2$ cells). The rat is given a time $T$ to accumulate as much food as possible. It can perform 4 actions: going up, down, left, right. \n",
    "\n",
    "The goal is to code an agent to solve this task that will learn by trial and error. We propose the following environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
    "        grid_size = grid_size+4\n",
    "        self.grid_size = grid_size\n",
    "        self.max_time = max_time\n",
    "        self.temperature = temperature\n",
    "\n",
    "        #board on which one plays\n",
    "        self.board = np.zeros((grid_size,grid_size))\n",
    "        self.position = np.zeros((grid_size,grid_size))\n",
    "\n",
    "        # coordinate of the cat\n",
    "        self.x = 0\n",
    "        self.y = 1\n",
    "\n",
    "        # self time\n",
    "        self.t = 0\n",
    "\n",
    "        self.scale=16\n",
    "\n",
    "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "    def draw(self,e):\n",
    "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
    "\n",
    "    def get_frame(self,t):\n",
    "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
    "        b[self.board>0,0] = 256\n",
    "        b[self.board < 0, 2] = 256\n",
    "        b[self.x,self.y,:]=256\n",
    "        b[-2:,:,:]=0\n",
    "        b[:,-2:,:]=0\n",
    "        b[:2,:,:]=0\n",
    "        b[:,:2,:]=0\n",
    "        \n",
    "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        self.to_draw[t,:,:,:]=b\n",
    "\n",
    "\n",
    "    def act(self, action):\n",
    "        \"\"\"This function returns the new state, reward and decides if the\n",
    "        game ends.\"\"\"\n",
    "\n",
    "        self.get_frame(int(self.t))\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        # self.position[-2:, :] = -1\n",
    "        self.position[:, -2:] = -1\n",
    "\n",
    "        self.position[self.x, self.y] = 1\n",
    "        if action == 0:\n",
    "            if self.x == self.grid_size-3:\n",
    "                self.x = self.x-1\n",
    "            else:\n",
    "                self.x = self.x + 1\n",
    "        elif action == 1:\n",
    "            if self.x == 2:\n",
    "                self.x = self.x+1\n",
    "            else:\n",
    "                self.x = self.x-1\n",
    "        elif action == 2:\n",
    "            if self.y == self.grid_size - 3:\n",
    "                self.y = self.y - 1\n",
    "            else:\n",
    "                self.y = self.y + 1\n",
    "        elif action == 3:\n",
    "            if self.y == 2:\n",
    "                self.y = self.y + 1\n",
    "            else:\n",
    "                self.y = self.y - 1\n",
    "        else:\n",
    "            RuntimeError('Error: action not recognized')\n",
    "\n",
    "        self.t = self.t + 1\n",
    "        reward = self.board[self.x, self.y]\n",
    "        self.board[self.x, self.y] = 0\n",
    "        game_over = self.t > self.max_time\n",
    "        state = np.concatenate((self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
    "\n",
    "        return state, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
    "\n",
    "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "\n",
    "\n",
    "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
    "\n",
    "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
    "\n",
    "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "        malus[bonus>0]=0\n",
    "\n",
    "        self.board = bonus + malus\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        # self.position[-2:, :] = -1\n",
    "        self.position[:, -2:] = -1\n",
    "        self.board[self.x,self.y] = 0\n",
    "        self.t = 0\n",
    "\n",
    "        state = np.concatenate((\n",
    "                               self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "\n",
    "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following elements are important because they correspond to the hyper parameters for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "size = 13\n",
    "T=200\n",
    "temperature=0.3\n",
    "epochs_train=50 # set small when debugging\n",
    "epochs_test=10 # set small when debugging\n",
    "\n",
    "# display videos\n",
    "def display_videos(name):\n",
    "    video = io.open(name, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    return '''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2__ Explain the use of the arrays ```position``` and ```board```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\textit{board}$ represents the environment as a grid where value $0.5$ stands for cheese, $0$ for a neutral cell and $-1$ for a poisonous cell. It defines the rewards in the different cells. The $\\textit{position}$ defines where the agent is on the grid and the bounds of the grid. This grid is padded to take in account the fact that the mouse sees $5\\times 5$ cells at once.\n",
    "Note : there was a mistake in the code of self.position as there was twice the line :self.position\\[-2:, :\\] = -1 and no occurence of self.position\\[:, -2:\\] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Question 3__ Implement a random Agent (only ```learned_act``` needs to be implemented):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super(RandomAgent, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def learned_act(self, s):\n",
    "        return np.random.randint(self.n_action, size=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__Question 4__ Visualize the game moves. You need to fill in the following function for the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent,env,epochs,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "        \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        ##### FILL IN HERE\n",
    "        state = env.reset()\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "        # The agent performs an action\n",
    "            action = agent.act(state, train = False)\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action)\n",
    "\n",
    "        # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose -reward\n",
    "            env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score = score + win-lose\n",
    "\n",
    "        print(\"Win/lose count {}/{}. Average score ({})\"\n",
    "              .format(win, lose, score/(1+e)))\n",
    "    print('Final score: '+str(score/epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win/lose count 5.0/18.0. Average score (-13.0)\n",
      "Win/lose count 10.0/9.0. Average score (-6.0)\n",
      "Win/lose count 7.0/16.0. Average score (-7.0)\n",
      "Win/lose count 9.0/14.0. Average score (-6.5)\n",
      "Win/lose count 11.0/10.0. Average score (-5.0)\n",
      "Win/lose count 14.0/26.0. Average score (-6.166666666666667)\n",
      "Win/lose count 13.0/11.0. Average score (-5.0)\n",
      "Win/lose count 12.5/17.0. Average score (-4.9375)\n",
      "Win/lose count 8.0/12.0. Average score (-4.833333333333333)\n",
      "Win/lose count 10.0/17.0. Average score (-5.05)\n",
      "Final score: -5.05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the game\n",
    "env = Environment(grid_size=size, max_time=T,temperature=temperature)\n",
    "\n",
    "# Initialize the agent!\n",
    "agent = RandomAgent()\n",
    "\n",
    "test(agent,env,epochs_test,prefix='random')\n",
    "HTML(display_videos('random0.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume here that $T=\\infty$.\n",
    "\n",
    "***\n",
    "__Question 5__ Let $\\pi$ be a policy, show that:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{\\pi}(s,a)=E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{\\pi}(s',a')]\n",
    "\\end{equation*}\n",
    "\n",
    "Then, show that for the optimal policy $\\pi^*$ (we assume its existence), the following holds: \n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{*}(s,a)=E_{s'\\sim \\pi^*(.|s,a)}[r(s,a)+\\gamma\\max_{a'}Q^{*}(s',a')].\n",
    "\\end{equation*}\n",
    "Finally, deduce that a plausible objective is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}(\\theta)=E_{s' \\sim \\pi^*(.|s,a)}\\Vert r+\\gamma\\max\\max_{a'}Q(s',a',\\theta)-Q(s,a,\\theta)\\Vert^{2}.\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "We have by definition \n",
    "\\begin{equation*}Q^\\pi(s,a)=E_{p^{\\pi}}[\\sum_{t\\leq T}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a] \\> .\n",
    "\\end{equation*}\n",
    "We simply have to split the sum into two parts as follows\n",
    "\\begin{equation*}Q^\\pi(s,a)=E_{p^{\\pi}}[r(s,a) + \\sum_{t = 1}^{T}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a]  \\\\\n",
    "Q^\\pi(s,a)=E_{p^{\\pi}}[r(s,a)] + E_{p^{\\pi}}[\\sum_{t = 1}^{T}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a]\n",
    "\\end{equation*}\n",
    "And we use the markov property to get\n",
    "\\begin{equation*}\n",
    "\\gamma Q^\\pi(s,a) = E_{(s',a')\\sim p(.|s,a)}[\\sum_{t = 1}^{T}\\gamma^{t}r(s',a')]\n",
    "\\end{equation*}\n",
    "therefore,\n",
    "\\begin{equation*}\n",
    "Q^{\\pi}(s,a)=E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{\\pi}(s',a')]\n",
    "\\end{equation*}\n",
    " \n",
    "Thus, the optimal Q function maximizing the reward defined as:\n",
    "\\begin{equation*}\n",
    "Q^*(s,a)=\\max_{\\pi}Q^\\pi(s,a) \\> .\n",
    "\\end{equation*}\n",
    "when inserted in the previous result gives us\n",
    "\\begin{equation*}\n",
    "Q^{*}(s,a)=E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{*}(s',a')]\n",
    "\\end{equation*}\n",
    "Also because this Q function is optimal we have $E_{(s',a')\\sim p(.|s,a)}[Q^{*}(s',a')] = E_{(s')\\sim p(.|s,a)}[ \\max_{a'} Q^{*}(s',a')]$ and get\n",
    "\\begin{equation*}\n",
    "Q^{*}(s,a)=E_{s'\\sim \\pi^*(.|s,a)}[r(s,a)+\\gamma\\max_{a'}Q^{*}(s',a')].\n",
    "\\end{equation*}\n",
    " \n",
    "We now have that the $Q_\\pi$-function is a fixed point of the Bellman operator $T$: \n",
    "\n",
    "\\begin{equation*}\n",
    "[T_\\pi Q](s,a)= r(s,a) + \\gamma \\sum_{s'\\in S} p(s'|s,a) Q(s', \\pi(s')) \\\\\n",
    "T_\\pi Q_\\pi = Q_\\pi\n",
    "\\end{equation*}\n",
    "\n",
    "And the $Q^*$-function is a fixed point of the optimal Bellman operator $T_*$: \n",
    "\n",
    "\\begin{equation*}\n",
    "[T_* Q](s,a)= r(s,a) + \\gamma \\sum_{s'\\in S} p(s'|s,a) max_{a' \\in A} Q(s', a') \\\\\n",
    "T_* Q^* = Q^*\n",
    "\\end{equation*}\n",
    " \n",
    "To find the fixed point of the optimal Bellman operator we can look at the expected distance between $T_* Q$ and $Q$. \n",
    "\\begin{equation*}\n",
    "\\mathcal{L}(\\theta)=E_{s' \\sim \\pi^*(.|s,a)}\\Vert [T_* Q](s,a, \\theta) -Q(s,a,\\theta)\\Vert^{2} =E_{s' \\sim \\pi^*(.|s,a)}\\Vert r+\\gamma\\max_{a'}Q(s',a',\\theta)-Q(s,a,\\theta)\\Vert^{2}.\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The DQN-learning algorithm relies on these derivations to train the parameters $\\theta$ of a Deep Neural Network:\n",
    "\n",
    "1. At the state $s_t$, select the action $a_t$ with best reward using $Q_t$ and store the results;\n",
    "\n",
    "2. Obtain the new state $s_{t+1}$ from the environment $p$;\n",
    "\n",
    "3. Store $(s_t,a_t,s_{t+1})$;\n",
    "\n",
    "4. Obtain $Q_{t+1}$ by minimizing  $\\mathcal{L}$ from a recovered batch from the previously stored results.\n",
    "\n",
    "***\n",
    "__Question 6__ Implement the class ```Memory``` that stores moves (in a replay buffer) via ```remember``` and provides a ```random_access``` to these. Specify a maximum memory size to avoid side effects. You can for example use a ```list()``` and set by default ```max_memory=100```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, max_memory=100):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "\n",
    "    def remember(self, m):\n",
    "        if len(self.memory) < self.max_memory:\n",
    "            self.memory.append(m)\n",
    "        else:\n",
    "            self.memory[:-1] = self.memory[1:]\n",
    "            self.memory[-1] = m\n",
    "\n",
    "    def random_access(self):\n",
    "        return np.random.randint(len(self.memory), size=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The pipeline we will use for training is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent,env,epoch,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "    loss = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will terminate\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose -reward\n",
    "\n",
    "            # Apply the reinforcement strategy\n",
    "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "\n",
    "        # Save as a mp4\n",
    "        if e % 10 == 0:\n",
    "            env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score += win-lose\n",
    "\n",
    "        print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "              .format(e, epoch, loss, win, lose, win-lose))\n",
    "        agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Question 7__ Implement the DQN training algorithm using a cascade of fully connected layers. You can use different learning rate, batch size or memory size parameters. In particular, the loss might oscillate while the player will start to win the games. You have to find a good criterium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Agent):\n",
    "    def __init__(self, grid_size,  epsilon = 0.1, memory_size=100, batch_size = 16,n_state=2):\n",
    "        super(DQN, self).__init__(epsilon = epsilon)\n",
    "\n",
    "        # Discount for Q learning\n",
    "        self.discount = 0.99\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        # number of state\n",
    "        self.n_state = n_state\n",
    "\n",
    "        # Memory\n",
    "        self.memory = Memory(memory_size)\n",
    "        \n",
    "        # Batch size when learning\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def learned_act(self, s):\n",
    "        predict_action = self.model.predict(s.reshape(1,5,5,self.n_state))\n",
    "        return np.argmax(predict_action)\n",
    "    \n",
    "    def reinforce(self, s_, n_s_, a_, r_, game_over_):\n",
    "        # Two steps: first memorize the states, second learn from the pool\n",
    "\n",
    "        self.memory.remember([s_, n_s_, a_, r_, game_over_])\n",
    "        \n",
    "        input_states = np.zeros((self.batch_size, 5,5,self.n_state))\n",
    "        target_q = np.zeros((self.batch_size, 4))\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            ind = self.memory.random_access()\n",
    "            input_states[i],next_state,action,reward,game_over_ = self.memory.memory[ind]\n",
    "            target_q[i] = self.model.predict(input_states[i].reshape(1,5,5,self.n_state))\n",
    "            \n",
    "            if game_over_:\n",
    "                target_q[i][action] = reward\n",
    "            else:\n",
    "                tmp = self.model.predict(next_state.reshape(1,5,5,self.n_state))\n",
    "                target_q[i][action] = reward + self.discount * (np.amax(tmp))\n",
    "        ######## FILL IN\n",
    "        # HINT: Clip the target to avoid exploiding gradients.. -- clipping is a bit tighter\n",
    "        clip = 8\n",
    "        target_q = np.clip(target_q, -clip, clip)\n",
    "\n",
    "        l = self.model.train_on_batch(input_states, target_q)\n",
    "\n",
    "\n",
    "        return l\n",
    "\n",
    "    def save(self,name_weights='model.h5',name_model='model.json'):\n",
    "        self.model.save_weights(name_weights, overwrite=True)\n",
    "        with open(name_model, \"w\") as outfile:\n",
    "            json.dump(self.model.to_json(), outfile)\n",
    "            \n",
    "    def load(self,name_weights='model.h5',name_model='model.json'):\n",
    "        with open(name_model, \"r\") as jfile:\n",
    "            model = model_from_json(json.load(jfile))\n",
    "        model.load_weights(name_weights)\n",
    "        model.compile(\"sgd\", \"mse\")\n",
    "        self.model = model\n",
    "\n",
    "            \n",
    "class DQN_FC(DQN):\n",
    "    def __init__(self, *args, lr=0.1,**kwargs):\n",
    "        super(DQN_FC, self).__init__( *args,**kwargs)\n",
    "        \n",
    "        input_layer = Input(shape=(5,5,self.n_state))\n",
    "        x = Flatten()(input_layer)\n",
    "        \n",
    "        x = Dense(512)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(128)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(128)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(64)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        output_layer = Dense(self.n_action)(x)\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        print(model.summary())\n",
    "        \n",
    "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mse\")\n",
    "        self.model = model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               26112     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 120,132\n",
      "Trainable params: 118,468\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 000/050 | Loss 0.1125 | Win/lose count 2.5/5.0 (-2.5)\n",
      "Epoch 001/050 | Loss 2.0448 | Win/lose count 9.0/10.0 (-1.0)\n",
      "Epoch 002/050 | Loss 0.8513 | Win/lose count 4.0/7.0 (-3.0)\n",
      "Epoch 003/050 | Loss 1.8637 | Win/lose count 8.0/8.0 (0.0)\n",
      "Epoch 004/050 | Loss 2.8917 | Win/lose count 3.0/5.0 (-2.0)\n",
      "Epoch 005/050 | Loss 0.8624 | Win/lose count 3.0/4.0 (-1.0)\n",
      "Epoch 006/050 | Loss 0.5739 | Win/lose count 11.0/11.0 (0.0)\n",
      "Epoch 007/050 | Loss 0.3216 | Win/lose count 1.0/2.0 (-1.0)\n",
      "Epoch 008/050 | Loss 0.3431 | Win/lose count 5.5/9.0 (-3.5)\n",
      "Epoch 009/050 | Loss 0.0148 | Win/lose count 6.0/11.0 (-5.0)\n",
      "Epoch 010/050 | Loss 0.5443 | Win/lose count 10.0/10.0 (0.0)\n",
      "Epoch 011/050 | Loss 0.0351 | Win/lose count 8.0/10.0 (-2.0)\n",
      "Epoch 012/050 | Loss 0.1850 | Win/lose count 5.5/5.0 (0.5)\n",
      "Epoch 013/050 | Loss 0.0193 | Win/lose count 8.5/9.0 (-0.5)\n",
      "Epoch 014/050 | Loss 0.0265 | Win/lose count 13.5/9.0 (4.5)\n",
      "Epoch 015/050 | Loss 0.4580 | Win/lose count 8.5/10.0 (-1.5)\n",
      "Epoch 016/050 | Loss 0.0205 | Win/lose count 10.0/11.0 (-1.0)\n",
      "Epoch 017/050 | Loss 0.0214 | Win/lose count 7.5/10.0 (-2.5)\n",
      "Epoch 018/050 | Loss 0.0267 | Win/lose count 12.0/12.0 (0.0)\n",
      "Epoch 019/050 | Loss 0.0074 | Win/lose count 7.5/10.0 (-2.5)\n",
      "Epoch 020/050 | Loss 0.0228 | Win/lose count 3.5/2.0 (1.5)\n",
      "Epoch 021/050 | Loss 0.0417 | Win/lose count 5.0/3.0 (2.0)\n",
      "Epoch 022/050 | Loss 0.0074 | Win/lose count 5.0/7.0 (-2.0)\n",
      "Epoch 023/050 | Loss 0.0159 | Win/lose count 8.0/8.0 (0.0)\n",
      "Epoch 024/050 | Loss 0.0430 | Win/lose count 4.5/5.0 (-0.5)\n",
      "Epoch 025/050 | Loss 0.0215 | Win/lose count 9.0/12.0 (-3.0)\n",
      "Epoch 026/050 | Loss 0.4105 | Win/lose count 10.0/12.0 (-2.0)\n",
      "Epoch 027/050 | Loss 0.0259 | Win/lose count 11.0/9.0 (2.0)\n",
      "Epoch 028/050 | Loss 0.2524 | Win/lose count 7.0/4.0 (3.0)\n",
      "Epoch 029/050 | Loss 0.0213 | Win/lose count 6.0/5.0 (1.0)\n",
      "Epoch 030/050 | Loss 0.0205 | Win/lose count 6.5/2.0 (4.5)\n",
      "Epoch 031/050 | Loss 0.2127 | Win/lose count 6.5/6.0 (0.5)\n",
      "Epoch 032/050 | Loss 0.0248 | Win/lose count 4.0/14.0 (-10.0)\n",
      "Epoch 033/050 | Loss 0.0227 | Win/lose count 2.0/5.0 (-3.0)\n",
      "Epoch 034/050 | Loss 0.0306 | Win/lose count 5.0/9.0 (-4.0)\n",
      "Epoch 035/050 | Loss 0.0124 | Win/lose count 9.0/5.0 (4.0)\n",
      "Epoch 036/050 | Loss 0.0319 | Win/lose count 7.0/2.0 (5.0)\n",
      "Epoch 037/050 | Loss 0.0209 | Win/lose count 5.5/8.0 (-2.5)\n",
      "Epoch 038/050 | Loss 0.0124 | Win/lose count 5.5/5.0 (0.5)\n",
      "Epoch 039/050 | Loss 0.0181 | Win/lose count 7.0/5.0 (2.0)\n",
      "Epoch 040/050 | Loss 0.0377 | Win/lose count 4.0/3.0 (1.0)\n",
      "Epoch 041/050 | Loss 0.0137 | Win/lose count 4.5/10.0 (-5.5)\n",
      "Epoch 042/050 | Loss 0.0051 | Win/lose count 3.0/5.0 (-2.0)\n",
      "Epoch 043/050 | Loss 0.0230 | Win/lose count 1.5/4.0 (-2.5)\n",
      "Epoch 044/050 | Loss 0.0195 | Win/lose count 2.5/1.0 (1.5)\n",
      "Epoch 045/050 | Loss 0.0142 | Win/lose count 4.0/4.0 (0.0)\n",
      "Epoch 046/050 | Loss 0.0129 | Win/lose count 3.5/4.0 (-0.5)\n",
      "Epoch 047/050 | Loss 0.0215 | Win/lose count 2.5/4.0 (-1.5)\n",
      "Epoch 048/050 | Loss 0.0304 | Win/lose count 4.5/6.0 (-1.5)\n",
      "Epoch 049/050 | Loss 0.0028 | Win/lose count 1.0/2.0 (-1.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_FC(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "train(agent, env, epochs_train, prefix='fc_train')\n",
    "HTML(display_videos('fc_train' + str(epochs_train-1 - (epochs_train-1)%10) + '.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__Question 8__ Implement the DQN training algorithm using a CNN (for example, 2 convolutional layers and one final fully connected layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_CNN(DQN):\n",
    "    def __init__(self, *args,lr=0.1,**kwargs):\n",
    "        super(DQN_CNN, self).__init__(*args,**kwargs)\n",
    "        \n",
    "        input_layer = Input(shape=(5,5,self.n_state))\n",
    "        x = Conv2D(32, (2, 2))(input_layer)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Conv2D(32, (2, 2))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(64)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        output_layer = Dense(self.n_action)(x)\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mse\")\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 4, 4, 32)          288       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 3, 3, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 23,684\n",
      "Trainable params: 23,428\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 000/050 | Loss 0.5469 | Win/lose count 4.5/1.0 (3.5)\n",
      "Epoch 001/050 | Loss 0.2365 | Win/lose count 2.0/3.0 (-1.0)\n",
      "Epoch 002/050 | Loss 0.3701 | Win/lose count 3.5/5.0 (-1.5)\n",
      "Epoch 003/050 | Loss 0.1099 | Win/lose count 0/1.0 (-1.0)\n",
      "Epoch 004/050 | Loss 0.0810 | Win/lose count 1.5/2.0 (-0.5)\n",
      "Epoch 005/050 | Loss 0.6106 | Win/lose count 3.5/5.0 (-1.5)\n",
      "Epoch 006/050 | Loss 2.1406 | Win/lose count 4.0/8.0 (-4.0)\n",
      "Epoch 007/050 | Loss 0.1242 | Win/lose count 4.5/9.0 (-4.5)\n",
      "Epoch 008/050 | Loss 0.0552 | Win/lose count 7.5/7.0 (0.5)\n",
      "Epoch 009/050 | Loss 0.0249 | Win/lose count 10.5/12.0 (-1.5)\n",
      "Epoch 010/050 | Loss 0.4726 | Win/lose count 7.5/9.0 (-1.5)\n",
      "Epoch 011/050 | Loss 0.0320 | Win/lose count 11.5/9.0 (2.5)\n",
      "Epoch 012/050 | Loss 0.0189 | Win/lose count 6.5/1.0 (5.5)\n",
      "Epoch 013/050 | Loss 0.0352 | Win/lose count 5.5/1.0 (4.5)\n",
      "Epoch 014/050 | Loss 0.0322 | Win/lose count 9.0/3.0 (6.0)\n",
      "Epoch 015/050 | Loss 0.3458 | Win/lose count 7.5/5.0 (2.5)\n",
      "Epoch 016/050 | Loss 0.0232 | Win/lose count 8.0/2.0 (6.0)\n",
      "Epoch 017/050 | Loss 0.0115 | Win/lose count 6.0/3.0 (3.0)\n",
      "Epoch 018/050 | Loss 0.0170 | Win/lose count 5.5/4.0 (1.5)\n",
      "Epoch 019/050 | Loss 0.0489 | Win/lose count 8.5/4.0 (4.5)\n",
      "Epoch 020/050 | Loss 0.1060 | Win/lose count 7.0/1.0 (6.0)\n",
      "Epoch 021/050 | Loss 0.0879 | Win/lose count 9.0/5.0 (4.0)\n",
      "Epoch 022/050 | Loss 0.0152 | Win/lose count 8.5/4.0 (4.5)\n",
      "Epoch 023/050 | Loss 0.0173 | Win/lose count 7.0/1.0 (6.0)\n",
      "Epoch 024/050 | Loss 0.0334 | Win/lose count 12.0/7.0 (5.0)\n",
      "Epoch 025/050 | Loss 0.0267 | Win/lose count 6.5/2.0 (4.5)\n",
      "Epoch 026/050 | Loss 0.0100 | Win/lose count 10.5/2.0 (8.5)\n",
      "Epoch 027/050 | Loss 0.0129 | Win/lose count 10.0/4.0 (6.0)\n",
      "Epoch 028/050 | Loss 0.0174 | Win/lose count 8.0/3.0 (5.0)\n",
      "Epoch 029/050 | Loss 0.0107 | Win/lose count 3.5/1.0 (2.5)\n",
      "Epoch 030/050 | Loss 0.0151 | Win/lose count 15.5/5.0 (10.5)\n",
      "Epoch 031/050 | Loss 0.0245 | Win/lose count 14.5/3.0 (11.5)\n",
      "Epoch 032/050 | Loss 0.0205 | Win/lose count 8.0/2.0 (6.0)\n",
      "Epoch 033/050 | Loss 0.0217 | Win/lose count 6.5/2.0 (4.5)\n",
      "Epoch 034/050 | Loss 0.0159 | Win/lose count 13.5/1.0 (12.5)\n",
      "Epoch 035/050 | Loss 0.0262 | Win/lose count 10.5/5.0 (5.5)\n",
      "Epoch 036/050 | Loss 0.0169 | Win/lose count 13.5/3.0 (10.5)\n",
      "Epoch 037/050 | Loss 0.0220 | Win/lose count 9.0/2.0 (7.0)\n",
      "Epoch 038/050 | Loss 0.0155 | Win/lose count 8.0/6.0 (2.0)\n",
      "Epoch 039/050 | Loss 0.0221 | Win/lose count 6.0/1.0 (5.0)\n",
      "Epoch 040/050 | Loss 0.0170 | Win/lose count 6.5/4.0 (2.5)\n",
      "Epoch 041/050 | Loss 0.0143 | Win/lose count 11.0/2.0 (9.0)\n",
      "Epoch 042/050 | Loss 0.0306 | Win/lose count 9.0/3.0 (6.0)\n",
      "Epoch 043/050 | Loss 0.0197 | Win/lose count 12.5/5.0 (7.5)\n",
      "Epoch 044/050 | Loss 0.0240 | Win/lose count 13.5/4.0 (9.5)\n",
      "Epoch 045/050 | Loss 0.0197 | Win/lose count 7.5/4.0 (3.5)\n",
      "Epoch 046/050 | Loss 0.0229 | Win/lose count 2.5/2.0 (0.5)\n",
      "Epoch 047/050 | Loss 0.3220 | Win/lose count 17.5/2.0 (15.5)\n",
      "Epoch 048/050 | Loss 0.0159 | Win/lose count 17.0/2.0 (15.0)\n",
      "Epoch 049/050 | Loss 0.0286 | Win/lose count 13.5/4.0 (9.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "train(agent,env,epochs_train,prefix='cnn_train')\n",
    "# HTML(display_videos('cnn_train90.mp4'))\n",
    "HTML(display_videos('cnn_train' + str(epochs_train-1 - (epochs_train-1)%10) + '.mp4'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__Question 9__ Test both algorithms and compare their performances. Which issue(s) do you observe? Observe also different behaviors by changing the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 32)          288       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 3, 3, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 23,684\n",
      "Trainable params: 23,428\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               26112     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 120,132\n",
      "Trainable params: 118,468\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Temperature : 0.1\n",
      "Test of the CNN\n",
      "Win/lose count 2.5/1.0. Average score (1.5)\n",
      "Win/lose count 0/0. Average score (0.75)\n",
      "Win/lose count 0.5/0. Average score (0.6666666666666666)\n",
      "Win/lose count 0.5/0. Average score (0.625)\n",
      "Win/lose count 2.0/4.0. Average score (0.1)\n",
      "Final score: 0.1\n",
      "Test of the FC\n",
      "Win/lose count 0.5/0. Average score (0.5)\n",
      "Win/lose count 0/0. Average score (0.25)\n",
      "Win/lose count 0/4.0. Average score (-1.1666666666666667)\n",
      "Win/lose count 1.0/0. Average score (-0.625)\n",
      "Win/lose count 0.5/1.0. Average score (-0.6)\n",
      "Final score: -0.6\n",
      "\n",
      "Temperature : 0.2\n",
      "Test of the CNN\n",
      "Win/lose count 2.0/1.0. Average score (1.0)\n",
      "Win/lose count 0/0. Average score (0.5)\n",
      "Win/lose count 1.0/1.0. Average score (0.3333333333333333)\n",
      "Win/lose count 0.5/0. Average score (0.375)\n",
      "Win/lose count 1.5/0. Average score (0.6)\n",
      "Final score: 0.6\n",
      "Test of the FC\n",
      "Win/lose count 0/1.0. Average score (-1.0)\n",
      "Win/lose count 1.5/2.0. Average score (-0.75)\n",
      "Win/lose count 2.0/5.0. Average score (-1.5)\n",
      "Win/lose count 0.5/2.0. Average score (-1.5)\n",
      "Win/lose count 0.5/1.0. Average score (-1.3)\n",
      "Final score: -1.3\n",
      "\n",
      "Temperature : 0.3\n",
      "Test of the CNN\n",
      "Win/lose count 0/0. Average score (0.0)\n",
      "Win/lose count 0.5/1.0. Average score (-0.25)\n",
      "Win/lose count 0.5/0. Average score (0.0)\n",
      "Win/lose count 0.5/0. Average score (0.125)\n",
      "Win/lose count 0.5/0. Average score (0.2)\n",
      "Final score: 0.2\n",
      "Test of the FC\n",
      "Win/lose count 1.0/1.0. Average score (0.0)\n",
      "Win/lose count 1.5/1.0. Average score (0.25)\n",
      "Win/lose count 0/0. Average score (0.16666666666666666)\n",
      "Win/lose count 1.0/5.0. Average score (-0.875)\n",
      "Win/lose count 1.0/0. Average score (-0.5)\n",
      "Final score: -0.5\n",
      "\n",
      "Temperature : 0.7\n",
      "Test of the CNN\n",
      "Win/lose count 2.5/0. Average score (2.5)\n",
      "Win/lose count 5.5/0. Average score (4.0)\n",
      "Win/lose count 0.5/0. Average score (2.8333333333333335)\n",
      "Win/lose count 0/1.0. Average score (1.875)\n",
      "Win/lose count 5.0/0. Average score (2.5)\n",
      "Final score: 2.5\n",
      "Test of the FC\n",
      "Win/lose count 1.0/1.0. Average score (0.0)\n",
      "Win/lose count 2.0/0. Average score (1.0)\n",
      "Win/lose count 0/1.0. Average score (0.3333333333333333)\n",
      "Win/lose count 2.0/0. Average score (0.75)\n",
      "Win/lose count 4.0/2.0. Average score (1.0)\n",
      "Final score: 1.0\n",
      "\n",
      "Temperature : 0.9\n",
      "Test of the CNN\n",
      "Win/lose count 5.5/0. Average score (5.5)\n",
      "Win/lose count 5.5/0. Average score (5.5)\n",
      "Win/lose count 7.0/2.0. Average score (5.333333333333333)\n",
      "Win/lose count 0.5/0. Average score (4.125)\n",
      "Win/lose count 0.5/0. Average score (3.4)\n",
      "Final score: 3.4\n",
      "Test of the FC\n",
      "Win/lose count 1.0/0. Average score (1.0)\n",
      "Win/lose count 1.5/2.0. Average score (0.25)\n",
      "Win/lose count 4.0/2.0. Average score (0.8333333333333334)\n",
      "Win/lose count 6.0/1.0. Average score (1.875)\n",
      "Win/lose count 3.0/0. Average score (2.1)\n",
      "Final score: 2.1\n"
     ]
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T,temperature=0.3)\n",
    "agent_cnn = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "agent_cnn.load(name_weights='cnn_trainmodel.h5',name_model='cnn_trainmodel.json')\n",
    "\n",
    "agent_fc = DQN_FC(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "agent_cnn.load(name_weights='fc_trainmodel.h5',name_model='fc_trainmodel.json')\n",
    "\n",
    "temperatures = [0.1,0.2,0.3,0.7,0.9]\n",
    "for t in temperatures:\n",
    "    print('\\nTemperature : ' + str(t))\n",
    "    env = Environment(grid_size=size, max_time=T,temperature=t)\n",
    "    print('Test of the CNN')\n",
    "    test(agent_cnn,env, 5,prefix='cnn_test')\n",
    "    print('Test of the FC')\n",
    "    test(agent_fc,env, 5,prefix='fc_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAFNltZGF0AAACnwYF//+b3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1MiAtIEguMjY0L01QRUctNCBBVkMgY29kZWMgLSBDb3B5bGVmdCAyMDAzLTIwMTcgLSBodHRwOi8vd3d3LnZpZGVvbGFuLm9yZy94MjY0Lmh0bWwgLSBvcHRpb25zOiBjYWJhYz0xIHJlZj0zIGRlYmxvY2s9MTowOjAgYW5hbHlzZT0weDE6MHgxMTEgbWU9aGV4IHN1Ym1lPTcgcHN5PTEgcHN5X3JkPTEuMDA6MC4wMCBtaXhlZF9yZWY9MSBtZV9yYW5nZT0xNiBjaHJvbWFfbWU9MSB0cmVsbGlzPTEgOHg4ZGN0PTAgY3FtPTAgZGVhZHpvbmU9MjEsMTEgZmFzdF9wc2tpcD0xIGNocm9tYV9xcF9vZmZzZXQ9NCB0aHJlYWRzPTYgbG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTI1IHNjZW5lY3V0PTQwIGludHJhX3JlZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAABWGWIhAA3//72h/gU2VgT/lm//Q1/3I/bj6z9cWMhBN9aryHowBkvtR/m/6xRbOmelAf34AiUAHNuGcKScOL/wKS3VvgUy2E5FDocUKhhR1ET/KB3ESWYTNewrlBiQT6GIRVQX79j7klr+4a7kQe9O0nIdLmoeSNDk3EBaHCDrGkmifo531Ce4+XMAAWPeUHqrZJkMbokm44z32EZEiVBQZ8oznOfDzjc6lVk2dc0YaR4BHzb2hnIlQ1IA+wqjMz1+AqwwAER0B0e2asGGb7VaAT616UZWrBhG16k2+TnlWD96jHC/jCL3bpbx8d/OmWk+noLOSkR/Jqg/nrBw0pWUBHx83V6pqk/T2E4YJFsnvg6iTt6sZChUyRPB7YDZwiPi4H5tJVv/iGVGB45yOqPZJYjoszs8UdwuH6hETGgNlxjiUGvyBudE/Gp8uRe5yNy+qt28P9QwAbdAAAALEGaImxDf/6nhAANPuM1+IRzg1/+EoVnxf/4R2kxf/4SiYH1lv51AOLITar4AAAADwGeQXkK/wAKzaO80yEGYwAAABtBmkQ8IZMphDP//p4QACDfEP8Ff+tOkbFWIvgAAAAQAZ5jakK/AAbolv4D6/gicQAAABhBmmVJ4Q8mUwIZ//6eEAAVH4nZ1ugZJg0AAAAYQZqGSeEPJlMCGf/+nhAAFI+J2dboGSYtAAAAGEGap0nhDyZTAhv//qeEAAUj406CtZlOZQAAABhBmshJ4Q8mUwIb//6nhAAE/+NOgrWZTm0AAAA6QZrsSeEPJlMCG//+p4QABLtCHfxCOsJ/+EjEHF//gzjF//hI1t+tu7B4FL6KnwKOFf4FL+0Os7h7gAAAABVBnwpFETwv/wAC4VRc0UyLc+iy0YkAAAAQAZ8pdEK/AAPhwwGSW/2+QAAAABABnytqQr8AA+LPCHjQ1s2AAAAAGUGbLUmoQWiZTAhv//6nhAAE2+jmgrWZTnUAAAAaQZtOSeEKUmUwId/+qZYAAmP0ugcP88/TmMEAAAASQZtySeEOiZTAh3/+qZYAAJWBAAAAEkGfkEURPC//AALpio3fp6gfIAAAAA8Bn690Qr8AA+MYeUNAzukAAAAQAZ+xakK/AAPiC851oYYmQQAAABNBm7ZJqEFomUwId//+qZYAAJWAAAAAEEGf1EURLC//AALpktm/ShYAAAAPAZ/zdEK/AAPjGHlDQM7pAAAAEAGf9WpCvwAD4gvOdaGGJkAAAAATQZv6SahBbJlMCHf//qmWAACVgQAAABBBnhhFFSwv/wAC6ZLZv0oXAAAADwGeN3RCvwAD4xh5Q0DO6QAAABABnjlqQr8AA+ILznWhhiZBAAAAE0GaPkmoQWyZTAh3//6plgAAlYAAAAAQQZ5cRRUsL/8AAumS2b9KFwAAAA8Bnnt0Qr8AA+MYeUNAzukAAAAQAZ59akK/AAPiC851oYYmQAAAABNBmmJJqEFsmUwId//+qZYAAJWAAAAAEEGegEUVLC//AALpktm/ShcAAAAPAZ6/dEK/AAPjGHlDQM7pAAAAEAGeoWpCvwAD4gvOdaGGJkEAAAATQZqmSahBbJlMCHf//qmWAACVgAAAABBBnsRFFSwv/wAC6ZLZv0oXAAAADwGe43RCvwAD4xh5Q0DO6QAAABABnuVqQr8AA+ILznWhhiZBAAAAE0Ga6kmoQWyZTAh3//6plgAAlYEAAAAQQZ8IRRUsL/8AAumS2b9KFgAAAA8Bnyd0Qr8AA+MYeUNAzukAAAAQAZ8pakK/AAPiC851oYYmQQAAABNBmy5JqEFsmUwId//+qZYAAJWAAAAAEEGfTEUVLC//AALpktm/ShYAAAAPAZ9rdEK/AAPjGHlDQM7pAAAAEAGfbWpCvwAD4gvOdaGGJkEAAAATQZtySahBbJlMCHf//qmWAACVgQAAABBBn5BFFSwv/wAC6ZLZv0oWAAAADwGfr3RCvwAD4xh5Q0DO6QAAABABn7FqQr8AA+ILznWhhiZBAAAAE0GbtkmoQWyZTAh3//6plgAAlYAAAAAQQZ/URRUsL/8AAumS2b9KFgAAAA8Bn/N0Qr8AA+MYeUNAzukAAAAQAZ/1akK/AAPiC851oYYmQAAAABNBm/pJqEFsmUwId//+qZYAAJWBAAAAEEGeGEUVLC//AALpktm/ShcAAAAPAZ43dEK/AAPjGHlDQM7pAAAAEAGeOWpCvwAD4gvOdaGGJkEAAAATQZo+SahBbJlMCHf//qmWAACVgAAAABBBnlxFFSwv/wAC6ZLZv0oXAAAADwGee3RCvwAD4xh5Q0DO6QAAABABnn1qQr8AA+ILznWhhiZAAAAAE0GaYkmoQWyZTAh3//6plgAAlYAAAAAQQZ6ARRUsL/8AAumS2b9KFwAAAA8Bnr90Qr8AA+MYeUNAzukAAAAQAZ6hakK/AAPiC851oYYmQQAAABNBmqZJqEFsmUwId//+qZYAAJWAAAAAEEGexEUVLC//AALpktm/ShcAAAAPAZ7jdEK/AAPjGHlDQM7pAAAAEAGe5WpCvwAD4gvOdaGGJkEAAAATQZrqSahBbJlMCHf//qmWAACVgQAAABBBnwhFFSwv/wAC6ZLZv0oWAAAADwGfJ3RCvwAD4xh5Q0DO6QAAABABnylqQr8AA+ILznWhhiZBAAAAE0GbLkmoQWyZTAh3//6plgAAlYAAAAAQQZ9MRRUsL/8AAumS2b9KFgAAAA8Bn2t0Qr8AA+MYeUNAzukAAAAQAZ9takK/AAPiC851oYYmQQAAABNBm3JJqEFsmUwId//+qZYAAJWBAAAAEEGfkEUVLC//AALpktm/ShYAAAAPAZ+vdEK/AAPjGHlDQM7pAAAAEAGfsWpCvwAD4gvOdaGGJkEAAAATQZu2SahBbJlMCHf//qmWAACVgAAAABBBn9RFFSwv/wAC6ZLZv0oWAAAADwGf83RCvwAD4xh5Q0DO6QAAABABn/VqQr8AA+ILznWhhiZAAAAAE0Gb+kmoQWyZTAh3//6plgAAlYEAAAAQQZ4YRRUsL/8AAumS2b9KFwAAAA8Bnjd0Qr8AA+MYeUNAzukAAAAQAZ45akK/AAPiC851oYYmQQAAABNBmj5JqEFsmUwId//+qZYAAJWAAAAAEEGeXEUVLC//AALpktm/ShcAAAAPAZ57dEK/AAPjGHlDQM7pAAAAEAGefWpCvwAD4gvOdaGGJkAAAAATQZpiSahBbJlMCHf//qmWAACVgAAAABBBnoBFFSwv/wAC6ZLZv0oXAAAADwGev3RCvwAD4xh5Q0DO6QAAABABnqFqQr8AA+ILznWhhiZBAAAAE0GapkmoQWyZTAh3//6plgAAlYAAAAAQQZ7ERRUsL/8AAumS2b9KFwAAAA8BnuN0Qr8AA+MYeUNAzukAAAAQAZ7lakK/AAPiC851oYYmQQAAABNBmupJqEFsmUwId//+qZYAAJWBAAAAEEGfCEUVLC//AALpktm/ShYAAAAPAZ8ndEK/AAPjGHlDQM7pAAAAEAGfKWpCvwAD4gvOdaGGJkEAAAATQZsuSahBbJlMCHf//qmWAACVgAAAABBBn0xFFSwv/wAC6ZLZv0oWAAAADwGfa3RCvwAD4xh5Q0DO6QAAABABn21qQr8AA+ILznWhhiZBAAAAE0GbckmoQWyZTAh3//6plgAAlYEAAAAQQZ+QRRUsL/8AAumS2b9KFgAAAA8Bn690Qr8AA+MYeUNAzukAAAAQAZ+xakK/AAPiC851oYYmQQAAABNBm7ZJqEFsmUwId//+qZYAAJWAAAAAEEGf1EUVLC//AALpktm/ShYAAAAPAZ/zdEK/AAPjGHlDQM7pAAAAEAGf9WpCvwAD4gvOdaGGJkAAAAATQZv6SahBbJlMCHf//qmWAACVgQAAABBBnhhFFSwv/wAC6ZLZv0oXAAAADwGeN3RCvwAD4xh5Q0DO6QAAABABnjlqQr8AA+ILznWhhiZBAAAAE0GaPkmoQWyZTAh3//6plgAAlYAAAAAQQZ5cRRUsL/8AAumS2b9KFwAAAA8Bnnt0Qr8AA+MYeUNAzukAAAAQAZ59akK/AAPiC851oYYmQAAAABNBmmJJqEFsmUwId//+qZYAAJWAAAAAEEGegEUVLC//AALpktm/ShcAAAAPAZ6/dEK/AAPjGHlDQM7pAAAAEAGeoWpCvwAD4gvOdaGGJkEAAAATQZqmSahBbJlMCHf//qmWAACVgAAAABBBnsRFFSwv/wAC6ZLZv0oXAAAADwGe43RCvwAD4xh5Q0DO6QAAABABnuVqQr8AA+ILznWhhiZBAAAAE0Ga6kmoQWyZTAh3//6plgAAlYEAAAAQQZ8IRRUsL/8AAumS2b9KFgAAAA8Bnyd0Qr8AA+MYeUNAzukAAAAQAZ8pakK/AAPiC851oYYmQQAAABNBmy5JqEFsmUwId//+qZYAAJWAAAAAEEGfTEUVLC//AALpktm/ShYAAAAPAZ9rdEK/AAPjGHlDQM7pAAAAEAGfbWpCvwAD4gvOdaGGJkEAAAATQZtySahBbJlMCHf//qmWAACVgQAAABBBn5BFFSwv/wAC6ZLZv0oWAAAADwGfr3RCvwAD4xh5Q0DO6QAAABABn7FqQr8AA+ILznWhhiZBAAAAE0GbtkmoQWyZTAh3//6plgAAlYAAAAAQQZ/URRUsL/8AAumS2b9KFgAAAA8Bn/N0Qr8AA+MYeUNAzukAAAAQAZ/1akK/AAPiC851oYYmQAAAABNBm/pJqEFsmUwId//+qZYAAJWBAAAAEEGeGEUVLC//AALpktm/ShcAAAAPAZ43dEK/AAPjGHlDQM7pAAAAEAGeOWpCvwAD4gvOdaGGJkEAAAATQZo+SahBbJlMCHf//qmWAACVgAAAABBBnlxFFSwv/wAC6ZLZv0oXAAAADwGee3RCvwAD4xh5Q0DO6QAAABABnn1qQr8AA+ILznWhhiZAAAAAE0GaYkmoQWyZTAh3//6plgAAlYAAAAAQQZ6ARRUsL/8AAumS2b9KFwAAAA8Bnr90Qr8AA+MYeUNAzukAAAAQAZ6hakK/AAPiC851oYYmQQAAABNBmqZJqEFsmUwId//+qZYAAJWAAAAAEEGexEUVLC//AALpktm/ShcAAAAPAZ7jdEK/AAPjGHlDQM7pAAAAEAGe5WpCvwAD4gvOdaGGJkEAAAATQZrqSahBbJlMCHf//qmWAACVgQAAABBBnwhFFSwv/wAC6ZLZv0oWAAAADwGfJ3RCvwAD4xh5Q0DO6QAAABABnylqQr8AA+ILznWhhiZBAAAAE0GbLkmoQWyZTAh3//6plgAAlYAAAAAQQZ9MRRUsL/8AAumS2b9KFgAAAA8Bn2t0Qr8AA+MYeUNAzukAAAAQAZ9takK/AAPiC851oYYmQQAAABNBm3JJqEFsmUwId//+qZYAAJWBAAAAEEGfkEUVLC//AALpktm/ShYAAAAPAZ+vdEK/AAPjGHlDQM7pAAAAEAGfsWpCvwAD4gvOdaGGJkEAAAATQZu2SahBbJlMCHf//qmWAACVgAAAABBBn9RFFSwv/wAC6ZLZv0oWAAAADwGf83RCvwAD4xh5Q0DO6QAAABABn/VqQr8AA+ILznWhhiZAAAAAE0Gb+kmoQWyZTAh3//6plgAAlYEAAAAQQZ4YRRUsL/8AAumS2b9KFwAAAA8Bnjd0Qr8AA+MYeUNAzukAAAAQAZ45akK/AAPiC851oYYmQQAAABNBmj5JqEFsmUwId//+qZYAAJWAAAAAEEGeXEUVLC//AALpktm/ShcAAAAPAZ57dEK/AAPjGHlDQM7pAAAAEAGefWpCvwAD4gvOdaGGJkAAAAASQZpiSahBbJlMCG///qeEAAEnAAAAEEGegEUVLC//AALpktm/ShcAAAAPAZ6/dEK/AAPjGHlDQM7pAAAAEAGeoWpCvwAD4gvOdaGGJkEAAAASQZqmSahBbJlMCGf//p4QAAR8AAAAEEGexEUVLC//AALpktm/ShcAAAAPAZ7jdEK/AAPjGHlDQM7pAAAAEAGe5WpCvwAD4gvOdaGGJkEAAAAaQZrpS6hCEFskRggoB/IB/YeAIV/+OEAAEXEAAAAnQZ8HRRUsK/8Cr2PtQcTdqsNJJuWqhgcstbvNKiCaLGHHo/gNAZ7AAAAAKgGfKGpCvwKvY+1BxN2qw0km5aqGByy1u80qIJosmUZ2ToiBXyEkPFztgAAADGBtb292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAAAfkAABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAALinRyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAAfkAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAABEAAAARAAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAAAQAAH5AAAAQAAAEAAAAACwJtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAADIAAAGUAFXEAAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAAqtbWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAAKbXN0YmwAAACVc3RzZAAAAAAAAAABAAAAhWF2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAABEAEQAEgAAABIAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAAvYXZjQwH0AA3/4QAXZ/QADZGbKCIR0IAAAAMAgAAAGQeKFMsBAAVo6+PESAAAABhzdHRzAAAAAAAAAAEAAADKAAACAAAAABRzdHNzAAAAAAAAAAEAAAABAAAGOGN0dHMAAAAAAAAAxQAAAAEAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAAEAAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAgAABAAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAgAAAAAAgAAAgAAAAAcc3RzYwAAAAAAAAABAAAAAQAAAMoAAAABAAADPHN0c3oAAAAAAAAAAAAAAMoAAAP/AAAAMAAAABMAAAAfAAAAFAAAABwAAAAcAAAAHAAAABwAAAA+AAAAGQAAABQAAAAUAAAAHQAAAB4AAAAWAAAAFgAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABcAAAAUAAAAEwAAABQAAAAXAAAAFAAAABMAAAAUAAAAFwAAABQAAAATAAAAFAAAABYAAAAUAAAAEwAAABQAAAAWAAAAFAAAABMAAAAUAAAAHgAAACsAAAAuAAAAFHN0Y28AAAAAAAAAAQAAADAAAABidWR0YQAAAFptZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0b28AAAAdZGF0YQAAAAEAAAAATGF2ZjU4LjEyLjEwMA==\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(display_videos('cnn_test0.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(display_videos('fc_test0.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the agent falls in a local minima which consists in switching between two cells which are empty, so essentially not playing in order to not lose after eating a piece of cheese.\n",
    "Also, it seems to me that the network still has  troubles remembering and taking in consideration the previously explored bits of the map.\n",
    "\n",
    "\n",
    "As expected, when we change the temperature, for high values the cheese is everywhere and the model has no difficulty winning, while with low temperatures, the cheese is rarer and the agent tend to achieve more equalities because it's not explorign far enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "The algorithm tends to not explore the map which can be an issue. We propose two ideas in order to encourage exploration:\n",
    "1. Incorporating a decreasing $\\epsilon$-greedy exploration. You can use the method ```set_epsilon```\n",
    "2. Append via the environment a new state that describes if a cell has been visited or not\n",
    "\n",
    "***\n",
    "__Question 10__ Design a new ```train_explore``` function and environment class ```EnvironmentExploring``` to tackle the issue of exploration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_explore(agent,env,epoch,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "    loss = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will terminate\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # we add the decreasing factor on epsilon\n",
    "            agent.set_epsilon(agent.epsilon * 0.9)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action, True)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose -reward\n",
    "\n",
    "            # Apply the reinforcement strategy\n",
    "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "\n",
    "        # Save as a mp4\n",
    "        if e % 10 == 0:\n",
    "            env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score += win-lose\n",
    "\n",
    "        print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "              .format(e, epoch, loss, win, lose, win-lose))\n",
    "        agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')\n",
    "        \n",
    "class EnvironmentExploring(object):\n",
    "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
    "        grid_size = grid_size+4\n",
    "        self.grid_size = grid_size\n",
    "        self.max_time = max_time\n",
    "        self.temperature = temperature\n",
    "        #board on which one plays\n",
    "        self.board = np.zeros((grid_size,grid_size))\n",
    "        self.position = np.zeros((grid_size,grid_size))\n",
    "        self.malus_position = np.zeros((grid_size,grid_size))\n",
    "\n",
    "        # coordinate of the cat\n",
    "        self.x = 0\n",
    "        self.y = 1\n",
    "\n",
    "        # self time\n",
    "        self.t = 0\n",
    "\n",
    "        self.scale=16\n",
    "\n",
    "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
    "\n",
    "    def draw(self,e):\n",
    "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
    "\n",
    "    def get_frame(self,t):\n",
    "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
    "        b[self.board>0,0] = 256\n",
    "        b[self.board < 0, 2] = 256\n",
    "        b[self.x,self.y,:]=256\n",
    "        b[-2:,:,:]=0\n",
    "        b[:,-2:,:]=0\n",
    "        b[:2,:,:]=0\n",
    "        b[:,:2,:]=0\n",
    "        \n",
    "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        self.to_draw[t,:,:,:]=b\n",
    "\n",
    "    def act(self, action, train = False):\n",
    "        \"\"\"This function returns the new state, reward and decides if the\n",
    "        game ends.\"\"\"\n",
    "\n",
    "        self.get_frame(int(self.t))\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        # self.position[-2:, :] = -1\n",
    "        self.position[:, -2:] = -1\n",
    "\n",
    "        self.position[self.x, self.y] = 1\n",
    "        if action == 0:\n",
    "            if self.x == self.grid_size-3:\n",
    "                self.x = self.x-1\n",
    "            else:\n",
    "                self.x = self.x + 1\n",
    "        elif action == 1:\n",
    "            if self.x == 2:\n",
    "                self.x = self.x+1\n",
    "            else:\n",
    "                self.x = self.x-1\n",
    "        elif action == 2:\n",
    "            if self.y == self.grid_size - 3:\n",
    "                self.y = self.y - 1\n",
    "            else:\n",
    "                self.y = self.y + 1\n",
    "        elif action == 3:\n",
    "            if self.y == 2:\n",
    "                self.y = self.y + 1\n",
    "            else:\n",
    "                self.y = self.y - 1\n",
    "        else:\n",
    "            RuntimeError('Error: action not recognized')\n",
    "\n",
    "        \n",
    "        game_over = self.t > self.max_time\n",
    "        \n",
    "        reward = 0\n",
    "        if train:\n",
    "            reward = -self.malus_position[self.x, self.y]\n",
    "        self.malus_position[self.x, self.y] += 0.1\n",
    "        if train:\n",
    "            # in training we increase the value of the rewards (this is equivalent to tuning the learning rate)\n",
    "            reward = reward + 5*self.board[self.x, self.y]\n",
    "        else:\n",
    "            reward = reward + self.board[self.x, self.y]\n",
    "        self.board[self.x, self.y] = 0\n",
    "\n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
    "        self.t = self.t + 1\n",
    "        \n",
    "        return state, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
    "\n",
    "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "\n",
    "\n",
    "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
    "\n",
    "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
    "\n",
    "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
    "        self.malus_position = np.zeros((self.grid_size,self.grid_size))\n",
    "\n",
    "        malus[bonus>0]=0\n",
    "\n",
    "        self.board = bonus + malus\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[:, -2:] = -1\n",
    "        self.board[self.x,self.y] = 0\n",
    "        self.t = 0\n",
    "\n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "\n",
    "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 5, 5, 3)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 32)          416       \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 3, 3, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 23,812\n",
      "Trainable params: 23,556\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 000/050 | Loss 0.1289 | Win/lose count 27.5/160.5999999999999 (-133.0999999999999)\n",
      "Epoch 001/050 | Loss 0.4077 | Win/lose count 35.0/116.19999999999996 (-81.19999999999996)\n",
      "Epoch 002/050 | Loss 10.1276 | Win/lose count 77.5/84.10000000000002 (-6.600000000000023)\n",
      "Epoch 003/050 | Loss 1.6374 | Win/lose count 75.0/90.2 (-15.200000000000003)\n",
      "Epoch 004/050 | Loss 0.5651 | Win/lose count 42.5/155.69999999999996 (-113.19999999999996)\n",
      "Epoch 005/050 | Loss 0.6785 | Win/lose count 27.5/105.89999999999993 (-78.39999999999993)\n",
      "Epoch 006/050 | Loss 1.0164 | Win/lose count 62.5/85.09999999999998 (-22.59999999999998)\n",
      "Epoch 007/050 | Loss 0.6763 | Win/lose count 40.0/101.20000000000002 (-61.20000000000002)\n",
      "Epoch 008/050 | Loss 0.5134 | Win/lose count 25.0/175.79999999999993 (-150.79999999999993)\n",
      "Epoch 009/050 | Loss 0.3891 | Win/lose count 35.0/137.2999999999999 (-102.2999999999999)\n",
      "Epoch 010/050 | Loss 0.2801 | Win/lose count 25.0/117.89999999999996 (-92.89999999999996)\n",
      "Epoch 011/050 | Loss 0.1439 | Win/lose count 35.0/134.4 (-99.4)\n",
      "Epoch 012/050 | Loss 1.4255 | Win/lose count 47.5/209.60000000000002 (-162.10000000000002)\n",
      "Epoch 013/050 | Loss 0.1421 | Win/lose count 82.5/42.80000000000003 (39.69999999999997)\n",
      "Epoch 014/050 | Loss 0.2686 | Win/lose count 70.0/54.60000000000005 (15.399999999999949)\n",
      "Epoch 015/050 | Loss 0.3966 | Win/lose count 52.5/85.90000000000002 (-33.40000000000002)\n",
      "Epoch 016/050 | Loss 0.2122 | Win/lose count 102.5/26.30000000000001 (76.19999999999999)\n",
      "Epoch 017/050 | Loss 0.9537 | Win/lose count 67.5/62.30000000000001 (5.199999999999989)\n",
      "Epoch 018/050 | Loss 0.3087 | Win/lose count 80.0/49.89999999999998 (30.100000000000023)\n",
      "Epoch 019/050 | Loss 0.2500 | Win/lose count 102.5/39.60000000000003 (62.89999999999997)\n",
      "Epoch 020/050 | Loss 0.1357 | Win/lose count 115.0/26.399999999999977 (88.60000000000002)\n",
      "Epoch 021/050 | Loss 0.8124 | Win/lose count 85.0/46.00000000000003 (38.99999999999997)\n",
      "Epoch 022/050 | Loss 0.0380 | Win/lose count 115.0/31.200000000000014 (83.79999999999998)\n",
      "Epoch 023/050 | Loss 0.4154 | Win/lose count 77.5/45.90000000000003 (31.599999999999973)\n",
      "Epoch 024/050 | Loss 0.0879 | Win/lose count 50.0/120.79999999999998 (-70.79999999999998)\n",
      "Epoch 025/050 | Loss 0.3952 | Win/lose count 80.0/52.00000000000002 (27.99999999999998)\n",
      "Epoch 026/050 | Loss 0.4388 | Win/lose count 90.0/40.80000000000007 (49.19999999999993)\n",
      "Epoch 027/050 | Loss 0.1383 | Win/lose count 80.0/100.70000000000002 (-20.700000000000017)\n",
      "Epoch 028/050 | Loss 0.1733 | Win/lose count 117.5/21.3 (96.2)\n",
      "Epoch 029/050 | Loss 0.0864 | Win/lose count 107.5/35.80000000000003 (71.69999999999996)\n",
      "Epoch 030/050 | Loss 0.2550 | Win/lose count 107.5/26.60000000000001 (80.89999999999999)\n",
      "Epoch 031/050 | Loss 0.0644 | Win/lose count 105.0/27.700000000000003 (77.3)\n",
      "Epoch 032/050 | Loss 0.0755 | Win/lose count 75.0/63.900000000000006 (11.099999999999994)\n",
      "Epoch 033/050 | Loss 0.0837 | Win/lose count 115.0/22.50000000000004 (92.49999999999996)\n",
      "Epoch 034/050 | Loss 0.5107 | Win/lose count 107.5/30.200000000000035 (77.29999999999997)\n",
      "Epoch 035/050 | Loss 0.1459 | Win/lose count 45.0/72.70000000000002 (-27.700000000000017)\n",
      "Epoch 036/050 | Loss 0.2825 | Win/lose count 75.0/48.00000000000002 (26.99999999999998)\n",
      "Epoch 037/050 | Loss 0.0984 | Win/lose count 127.5/38.70000000000001 (88.79999999999998)\n",
      "Epoch 038/050 | Loss 0.1075 | Win/lose count 85.0/32.0 (53.0)\n",
      "Epoch 039/050 | Loss 0.4541 | Win/lose count 95.0/31.300000000000022 (63.699999999999974)\n",
      "Epoch 040/050 | Loss 0.7475 | Win/lose count 125.0/16.999999999999993 (108.0)\n",
      "Epoch 041/050 | Loss 0.4343 | Win/lose count 105.0/32.79999999999998 (72.20000000000002)\n",
      "Epoch 042/050 | Loss 0.2704 | Win/lose count 100.0/27.800000000000022 (72.19999999999997)\n",
      "Epoch 043/050 | Loss 0.3205 | Win/lose count 112.5/54.60000000000007 (57.89999999999993)\n",
      "Epoch 044/050 | Loss 0.2361 | Win/lose count 132.5/9.799999999999986 (122.70000000000002)\n",
      "Epoch 045/050 | Loss 0.0586 | Win/lose count 122.5/20.099999999999994 (102.4)\n",
      "Epoch 046/050 | Loss 0.1478 | Win/lose count 92.5/29.70000000000004 (62.79999999999996)\n",
      "Epoch 047/050 | Loss 0.1866 | Win/lose count 105.0/14.099999999999989 (90.9)\n",
      "Epoch 048/050 | Loss 0.2019 | Win/lose count 127.5/12.59999999999999 (114.9)\n",
      "Epoch 049/050 | Loss 0.2332 | Win/lose count 142.5/18.099999999999994 (124.4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "env = EnvironmentExploring(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32,n_state=3)\n",
    "train_explore(agent, env, epochs_train, prefix='cnn_train_explore')\n",
    "HTML(display_videos('cnn_train_explore' + str(epochs_train-1 - (epochs_train-1)%10) + '.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win/lose count 23.0/1.0. Average score (22.0)\n",
      "Win/lose count 20.0/1.0. Average score (20.5)\n",
      "Win/lose count 28.0/1.0. Average score (22.666666666666668)\n",
      "Win/lose count 23.0/1.0. Average score (22.5)\n",
      "Win/lose count 24.0/0. Average score (22.8)\n",
      "Win/lose count 26.0/2.0. Average score (23.0)\n",
      "Win/lose count 23.5/2.0. Average score (22.785714285714285)\n",
      "Win/lose count 25.0/1.0. Average score (22.9375)\n",
      "Win/lose count 15.0/7.0. Average score (21.27777777777778)\n",
      "Win/lose count 21.0/2.0. Average score (21.05)\n",
      "Final score: 21.05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "test(agent,env,epochs_test,prefix='cnn_test_explore')\n",
    "HTML(display_videos('cnn_test_explore0.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method with an exploration memory is achieving very good results and manages to find almost all the pieces of cheese. This will allow us to transform this reinforcement problem in a supervised learning problem by giving us the opportunity to generate winning games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__BONUS question__ Use the expert DQN from the previous question to generate some winning games. Train a model that mimicks its behavior. Compare the performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win/lose count 23.5/1.0. Average score (22.5)\n",
      "Win/lose count 24.5/2.0. Average score (22.5)\n",
      "Win/lose count 25.5/1.0. Average score (23.166666666666668)\n",
      "Win/lose count 27.0/0. Average score (24.125)\n",
      "Win/lose count 22.0/1.0. Average score (23.5)\n",
      "Final score: 23.5\n"
     ]
    }
   ],
   "source": [
    "def save_for_training(state,action):\n",
    "    state_string = ''\n",
    "    w,h,d = state.shape\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            for k in range(d):\n",
    "                state_string += str(state[i,j,k]) + '/'\n",
    "            state_string = state_string[:-1]\n",
    "            state_string += '_'\n",
    "        state_string = state_string[:-1]\n",
    "        state_string += ' '\n",
    "    state_string = state_string[:-1]\n",
    "    with open('states.txt','a') as f:\n",
    "        f.write(state_string + '\\n')\n",
    "        f.close()\n",
    "    action_string = str(action)\n",
    "    with open('actions.txt','a') as f:\n",
    "        f.write(action_string + '\\n')\n",
    "        f.close()\n",
    "\n",
    "def generate_GT(trained_agent,env,epoch,prefix='mimick'):\n",
    "    with open('states.txt','w') as f:\n",
    "        f.close()\n",
    "    with open('actions.txt','w') as f:\n",
    "        f.close()\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "    loss = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will terminate\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "        # The agent performs an action\n",
    "            action = agent.act(state, train = False)\n",
    "            save_for_training(state,action)\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action)\n",
    "\n",
    "        # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose -reward\n",
    "            env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score = score + win-lose\n",
    "\n",
    "        print(\"Win/lose count {}/{}. Average score ({})\"\n",
    "              .format(win, lose, score/(1+e)))\n",
    "    print('Final score: '+str(score/epoch)) \n",
    "    \n",
    "generate_GT(agent, env, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous solver we generate and save some training data for the supervised learning. Each state is saved in a file named 'states.txt' and the corresponding ground, truth correct action to take, is saved in 'actions.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data():\n",
    "    states = []\n",
    "    actions = []\n",
    "    with open('states.txt','r') as f:\n",
    "        string_states = f.read()\n",
    "        f.close()\n",
    "    string_states = string_states.splitlines()[:-1]\n",
    "    for string in string_states:\n",
    "        state = []\n",
    "        for a in string.split(' '):\n",
    "            tmp1 = []\n",
    "            for b in a.split('_'):\n",
    "                tmp2 = []\n",
    "                for c in b.split('/'):\n",
    "                    if c != '':\n",
    "                        tmp2.append(float(c))\n",
    "                tmp1.append(tmp2)\n",
    "            state.append(tmp1)\n",
    "        states.append(state)\n",
    "    with open('actions.txt','r') as f:\n",
    "        string_actions = f.read()\n",
    "        f.close()\n",
    "    string_actions = string_actions.splitlines()[:-1]\n",
    "    for a in string_actions:\n",
    "        tmp = np.zeros(4)\n",
    "        tmp[int(a)] = 1\n",
    "        actions.append(tmp)\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    return states,actions\n",
    "\n",
    "States, Actions = load_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training data and store it in the variables States and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 5, 5, 3)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 32)          416       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 3, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 3, 3, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 23,812\n",
      "Trainable params: 23,556\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class DQN_mimick(DQN):\n",
    "    def __init__(self, *args,lr=0.1,**kwargs):\n",
    "        super(DQN_mimick, self).__init__(*args,**kwargs)\n",
    "        \n",
    "        input_layer = Input(shape=(5,5,self.n_state))\n",
    "        x = Conv2D(32, (2, 2))(input_layer)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Conv2D(32, (2, 2))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(64)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        output_layer = Dense(self.n_action)(x)\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mae\")\n",
    "        self.model = model\n",
    "    \n",
    "    def train(self, data, ground_truth):\n",
    "        l = self.model.train_on_batch(data, ground_truth)\n",
    "        return l\n",
    "        \n",
    "        \n",
    "agent_mimick = DQN_mimick(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32, n_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created an agent that will imitate the previous expert one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/100 | Loss 7.6306 \n",
      "Epoch 001/100 | Loss 7.5313 \n",
      "Epoch 002/100 | Loss 7.3523 \n",
      "Epoch 003/100 | Loss 7.2407 \n",
      "Epoch 004/100 | Loss 7.0640 \n",
      "Epoch 005/100 | Loss 7.0702 \n",
      "Epoch 006/100 | Loss 6.9625 \n",
      "Epoch 007/100 | Loss 6.7933 \n",
      "Epoch 008/100 | Loss 6.7886 \n",
      "Epoch 009/100 | Loss 6.6184 \n",
      "Epoch 010/100 | Loss 6.6474 \n",
      "Epoch 011/100 | Loss 6.5059 \n",
      "Epoch 012/100 | Loss 6.5851 \n",
      "Epoch 013/100 | Loss 6.3497 \n",
      "Epoch 014/100 | Loss 6.2145 \n",
      "Epoch 015/100 | Loss 6.1126 \n",
      "Epoch 016/100 | Loss 6.3734 \n",
      "Epoch 017/100 | Loss 6.1527 \n",
      "Epoch 018/100 | Loss 5.9120 \n",
      "Epoch 019/100 | Loss 6.1262 \n",
      "Epoch 020/100 | Loss 5.9495 \n",
      "Epoch 021/100 | Loss 5.9796 \n",
      "Epoch 022/100 | Loss 5.8635 \n",
      "Epoch 023/100 | Loss 5.8084 \n",
      "Epoch 024/100 | Loss 5.8407 \n",
      "Epoch 025/100 | Loss 5.5833 \n",
      "Epoch 026/100 | Loss 5.8042 \n",
      "Epoch 027/100 | Loss 5.5735 \n",
      "Epoch 028/100 | Loss 5.5506 \n",
      "Epoch 029/100 | Loss 5.5122 \n",
      "Epoch 030/100 | Loss 5.4105 \n",
      "Epoch 031/100 | Loss 5.4271 \n",
      "Epoch 032/100 | Loss 5.5604 \n",
      "Epoch 033/100 | Loss 5.1969 \n",
      "Epoch 034/100 | Loss 5.2106 \n",
      "Epoch 035/100 | Loss 5.3292 \n",
      "Epoch 036/100 | Loss 5.2733 \n",
      "Epoch 037/100 | Loss 5.3409 \n",
      "Epoch 038/100 | Loss 5.2290 \n",
      "Epoch 039/100 | Loss 5.3139 \n",
      "Epoch 040/100 | Loss 5.0690 \n",
      "Epoch 041/100 | Loss 5.1150 \n",
      "Epoch 042/100 | Loss 5.1827 \n",
      "Epoch 043/100 | Loss 5.1273 \n",
      "Epoch 044/100 | Loss 5.0638 \n",
      "Epoch 045/100 | Loss 5.0063 \n",
      "Epoch 046/100 | Loss 5.1043 \n",
      "Epoch 047/100 | Loss 5.0032 \n",
      "Epoch 048/100 | Loss 4.8729 \n",
      "Epoch 049/100 | Loss 4.9362 \n",
      "Epoch 050/100 | Loss 4.9651 \n",
      "Epoch 051/100 | Loss 4.8804 \n",
      "Epoch 052/100 | Loss 4.8112 \n",
      "Epoch 053/100 | Loss 4.7507 \n",
      "Epoch 054/100 | Loss 4.7958 \n",
      "Epoch 055/100 | Loss 4.8261 \n",
      "Epoch 056/100 | Loss 4.7202 \n",
      "Epoch 057/100 | Loss 4.5995 \n",
      "Epoch 058/100 | Loss 4.7764 \n",
      "Epoch 059/100 | Loss 4.7150 \n",
      "Epoch 060/100 | Loss 4.6011 \n",
      "Epoch 061/100 | Loss 4.6994 \n",
      "Epoch 062/100 | Loss 4.4647 \n",
      "Epoch 063/100 | Loss 4.5141 \n",
      "Epoch 064/100 | Loss 4.4113 \n",
      "Epoch 065/100 | Loss 4.5496 \n",
      "Epoch 066/100 | Loss 4.3774 \n",
      "Epoch 067/100 | Loss 4.6030 \n",
      "Epoch 068/100 | Loss 4.4317 \n",
      "Epoch 069/100 | Loss 4.2341 \n",
      "Epoch 070/100 | Loss 4.3120 \n",
      "Epoch 071/100 | Loss 4.4365 \n",
      "Epoch 072/100 | Loss 4.3293 \n",
      "Epoch 073/100 | Loss 4.4060 \n",
      "Epoch 074/100 | Loss 4.1984 \n",
      "Epoch 075/100 | Loss 4.3761 \n",
      "Epoch 076/100 | Loss 4.1502 \n",
      "Epoch 077/100 | Loss 4.2156 \n",
      "Epoch 078/100 | Loss 4.2604 \n",
      "Epoch 079/100 | Loss 4.2346 \n",
      "Epoch 080/100 | Loss 4.0202 \n",
      "Epoch 081/100 | Loss 4.0360 \n",
      "Epoch 082/100 | Loss 4.1613 \n",
      "Epoch 083/100 | Loss 4.1328 \n",
      "Epoch 084/100 | Loss 4.1918 \n",
      "Epoch 085/100 | Loss 4.2224 \n",
      "Epoch 086/100 | Loss 4.0825 \n",
      "Epoch 087/100 | Loss 4.1143 \n",
      "Epoch 088/100 | Loss 3.8436 \n",
      "Epoch 089/100 | Loss 4.0218 \n",
      "Epoch 090/100 | Loss 4.0744 \n",
      "Epoch 091/100 | Loss 4.0561 \n",
      "Epoch 092/100 | Loss 3.8981 \n",
      "Epoch 093/100 | Loss 3.9080 \n",
      "Epoch 094/100 | Loss 3.9767 \n",
      "Epoch 095/100 | Loss 4.0610 \n",
      "Epoch 096/100 | Loss 4.0089 \n",
      "Epoch 097/100 | Loss 3.8833 \n",
      "Epoch 098/100 | Loss 4.0514 \n",
      "Epoch 099/100 | Loss 3.9395 \n"
     ]
    }
   ],
   "source": [
    "def train_mimick(agent,data,ground_truth,epoch,prefix='mimick_'):\n",
    "    for e in range(epoch):\n",
    "        loss = 0\n",
    "        indeces = np.arange(data.shape[0])\n",
    "        np.random.shuffle(indeces)\n",
    "        batch_size = 32\n",
    "        for b_ind in range(int(data.shape[0]/batch_size) + 1):\n",
    "            batch = indeces[b_ind * batch_size: min((b_ind + 1) * batch_size, data.shape[0])]\n",
    "            batch_data = data[batch,:,:,:]\n",
    "            batch_gt = ground_truth[batch]\n",
    "            loss += agent.train(batch_data,batch_gt)\n",
    "        \n",
    "        print(\"Epoch {:03d}/{:03d} | Loss {:.4f} \"\n",
    "              .format(e, epoch, loss))\n",
    "        agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')\n",
    "\n",
    "train_mimick(agent_mimick, States, Actions, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win/lose count 24.0/8.0. Average score (16.0)\n",
      "Win/lose count 18.0/3.0. Average score (15.5)\n",
      "Win/lose count 16.0/1.0. Average score (15.333333333333334)\n",
      "Win/lose count 25.0/10.0. Average score (15.25)\n",
      "Win/lose count 21.5/4.0. Average score (15.7)\n",
      "Win/lose count 18.5/4.0. Average score (15.5)\n",
      "Win/lose count 17.5/4.0. Average score (15.214285714285714)\n",
      "Win/lose count 22.5/7.0. Average score (15.25)\n",
      "Win/lose count 24.5/5.0. Average score (15.722222222222221)\n",
      "Win/lose count 16.0/9.0. Average score (14.85)\n",
      "Final score: 14.85\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = EnvironmentExploring(grid_size=size, max_time=T, temperature=0.3)\n",
    "test(agent_mimick,env,epochs_test,prefix='mimick_test_explore')\n",
    "HTML(display_videos('mimick_test_explore0.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that even with a small amount of data and far less game sessions, the algorithm captured most of the strategy and achieves close results in terms of performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
